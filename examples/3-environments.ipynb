{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Create a custom Environment\n",
    "\n",
    "In this tutorial, we will go over `Environment` objects, how they work and how to build a custom subclass to implement your own task design.\n",
    "\n",
    "Let's start by importing what we need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cell using LOCAL initialization...\n",
      "All packages imported.\n",
      "pytorch version: 2.0.1\n",
      "numpy version: 1.23.0\n",
      "motornet version: 0.2.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from IPython import get_ipython\n",
    "\n",
    "implementation = \"motorchnet\"\n",
    "\n",
    "\n",
    "colab_env = 'google.colab' in str(get_ipython()) if hasattr(__builtins__,'__IPYTHON__') else False\n",
    "colab_initialized = True if os.path.exists(\"MotorNet\") else False\n",
    "\n",
    "if colab_env and not colab_initialized:\n",
    "  !git clone https://github.com/OlivierCodol/MotorNet\n",
    "  sys.path.append('MotorNet')\n",
    "  print(\"Running cell using COLAB initialization...\")\n",
    "elif colab_env and colab_initialized:\n",
    "  print(\"Already initialized using COLAB initialization.\")\n",
    "else:\n",
    "  paths = [p for p in sys.path if os.path.exists(p)]\n",
    "  local_initialized = True if [p for p in paths if implementation in os.listdir(p)] else False\n",
    "  if local_initialized:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    print(\"Already initialized using LOCAL initialization.\")\n",
    "  else:\n",
    "    path = [p for p in paths if p.__contains__(\"examples\")]\n",
    "    if len(path) != 1:\n",
    "      raise ValueError(\"Path to MotorNet could not be determined with certainty.\")\n",
    "    else:\n",
    "       path = path[0]\n",
    "    sys.path.append(os.path.dirname(path[:path.rfind('examples')]))\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    print(\"Running cell using LOCAL initialization...\")\n",
    "\n",
    "if implementation == \"motorchnet\":\n",
    "  import motorchnet as mn\n",
    "else:\n",
    "  import motornet as mn\n",
    "\n",
    "\n",
    "print('All packages imported.')\n",
    "print('pytorch version: ' + th.__version__)\n",
    "print('numpy version: ' + np.__version__)\n",
    "print('motornet version: ' + mn.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "# I. Useful methods in `Environment` objects\n",
    "\n",
    "Several methods are useful to assess what your task object currently contains.\n",
    "- The `print_attributes` method will print all attributes held by the `Environment` instance as well as their current value.\n",
    "- The `get_attributes` method will fetch those attributes, and return two lists: one with the name of each attribute, and one with the associated value of each attribute.\n",
    "\n",
    "First, let's import a built-in `Environment` object and create an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "effector = mn.effector.ReluPointMass24()\n",
    "env = mn.environment.Environment(effector=effector, name='env')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_destination:  ~T_destination\n",
      "action_frame_stacking:  0\n",
      "action_noise:  [0.0, 0.0, 0.0, 0.0]\n",
      "action_space:  Box(0.0, 1.0, (4,), float32)\n",
      "call_super_init:  False\n",
      "delay_range:  [0, 0]\n",
      "device:  cpu\n",
      "differentiable:  True\n",
      "dt:  0.01\n",
      "dump_patches:  False\n",
      "elapsed:  0.0\n",
      "goal:  tensor([[0., 0.]])\n",
      "max_ep_duration:  1.0\n",
      "metadata:  {'render_modes': []}\n",
      "n_muscles:  4\n",
      "np_random:  Generator(PCG64)\n",
      "nq_init:  None\n",
      "obs_buffer:  {'proprioception': [tensor([[2.2041, 3.5503, 2.2970, 3.6087, 0.0000, 0.0000, 0.0000, 0.0000]])], 'vision': [tensor([[0.9683, 0.0523]])], 'action': []}\n",
      "obs_noise:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "observation_space:  Box(-inf, inf, (12,), float32)\n",
      "proprioception_delay:  1\n",
      "proprioception_noise:  [0.0]\n",
      "q_init:  None\n",
      "render_mode:  None\n",
      "reward_range:  (-inf, inf)\n",
      "seed:  40823744835879118475264510630667754255\n",
      "space_dim:  2\n",
      "spec:  None\n",
      "states:  {'joint': tensor([[0.9683, 0.0523, 0.0000, 0.0000]]), 'cartesian': tensor([[0.9683, 0.0523, 0.0000, 0.0000]]), 'muscle': tensor([[[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [2.2041, 3.5503, 2.2970, 3.6087],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]]]), 'geometry': tensor([[[ 2.2041,  3.5503,  2.2970,  3.6087],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.4681,  0.8361, -0.4491,  0.8225],\n",
      "         [-0.8837, -0.5486,  0.8935,  0.5687]]]), 'fingertip': tensor([[0.9683, 0.0523]])}\n",
      "training:  True\n",
      "vision_delay:  1\n",
      "vision_noise:  [0.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env.print_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination', 'action_frame_stacking', 'action_noise', 'call_super_init', 'delay_range', 'device', 'differentiable', 'dt', 'dump_patches', 'elapsed', 'max_ep_duration', 'metadata', 'n_muscles', 'nq_init', 'obs_noise', 'proprioception_delay', 'proprioception_noise', 'q_init', 'render_mode', 'reward_range', 'seed', 'space_dim', 'spec', 'training', 'vision_delay', 'vision_noise']\n",
      "[~T_destination, 0, [0.0, 0.0, 0.0, 0.0], False, [0, 0], device(type='cpu'), True, 0.01, False, 0.0, 1.0, {'render_modes': []}, 4, None, [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 1, [0.0], None, None, (-inf, inf), 40823744835879118475264510630667754255, 2, None, True, 1, [0.0]]\n"
     ]
    }
   ],
   "source": [
    "attr_names, attr_values = env.get_attributes()\n",
    "print(attr_names)\n",
    "print(attr_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# II. Initializing an `Environment` object\n",
    "\n",
    "Once created, an `Environment` requires to be initialized. This can be done using the `Environment.reset()` method, which outputs an initial observation tensor and info dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs shape:           torch.Size([7, 12])\n",
      "\n",
      "\n",
      "states: \n",
      "\t\t\tjoint shape:      torch.Size([7, 4])\n",
      "\t\t\tcartesian shape:  torch.Size([7, 4])\n",
      "\t\t\tmuscle shape:     torch.Size([7, 4, 4])\n",
      "\t\t\tgeometry shape:   torch.Size([7, 4, 4])\n",
      "\t\t\tfingertip shape:  torch.Size([7, 2])\n",
      "action shape:        torch.Size([7, 4])\n",
      "noisy action shape:  torch.Size([7, 4])\n",
      "goal shape:          torch.Size([7, 2])\n"
     ]
    }
   ],
   "source": [
    "obs, info = env.reset(batch_size=7)\n",
    "\n",
    "print(\"obs shape:          \", obs.shape, end=\"\\n\\n\\n\")\n",
    "\n",
    "for key, val in info.items():\n",
    "  if type(val) is dict:\n",
    "    print(key + \": \")\n",
    "    for k, v in val.items():\n",
    "      print(\"\\t\\t\\t\" + k + \" shape:\" + \" \" * (10-len(k)), v.shape)\n",
    "  else:\n",
    "    print(key + \" shape:\" + \" \" * (13-len(key)), val.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The breakdown of this content is already detailed at the end of the previous tutorial on states, but this is repeated here for convenience.\n",
    "\n",
    "We can see that the `Effector` states are carried over to the `info` dictionary. In addition, the action, and its noisy version are also available, with shape `(batch_size, n_muscles)`. Additionally, the goal attribute, which is held at the `Environment` level, is also returned.\n",
    "\n",
    "The observation vector dimensionality is `(batch_size, n_observations)`, with the second dimension being an arbitrary combination of `Effector` states and other information that the user deems relevant for the network to receive as input. These observations can be potentially noised and/or time-delayed. By default, this is the goal, the fingertip state, the normalized muscle length for each muscle and normalized muscle elocity for each muscle. Since there are five muscles in this effector, and the goal and fingertip are 2D cartesian position vectors, this yields `n_observations = 2 + 2 + 5 + 5 = 14`. The content of the observation vector, and whether noise and/or time-delay is applied is defined when building the `Environment` class. This is further detailed in the follow-up `3-environments.ipynb` tutorial.\n",
    "\n",
    "**Importantly**, users familiar with reinforcement learning (RL) toolboxes will notice that this API design matches the standardized API used for RL packages and pipelines. This is not coincidental. The RL community has been relying on open-source packages and cross-compatible pipelines for years now, and abiding by a similar API design ensures that the insights and best practices learnt through this experience is translated here. Another important feature is that it enables compatibility between this package and packages implementing state-of-the-art RL agents.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# III. Subclassing an `Environment` object\n",
    "\n",
    "Now let's try to build our own task design. To do so, we will go over a simple subclassing process to create a custom environment. The task we will try to build is a reaching task from a random starting position in the full joint space to a random target in that joint space.\n",
    "\n",
    "**NOTE:** The `motornet.environment.Environment` class shares a lot of similarity with the [`gymnasium.Env`](https://gymnasium.farama.org/api/env/#gymnasium-env) class from the popular [`gymnasium`](https://gymnasium.farama.org) package. Getting familiar with this package will provide much insight into the logic of this class for `motornet`. Conversely, people already familiar with `gymnasium` should quickly be able to grasp how the `motornet.Environment` class works.\n",
    "\n",
    "**NOTE 2:** Actually, the `motornet.environment.Environment` class is a subclass of `gymnasium.Env`, so packages compatible with `gymnasium` should also be compatible with `motornet`.\n",
    "\n",
    "## III. 1. Inheritence of the `Environment` subclass\n",
    "The base class for environments is `mn.environment.Environment`, so we will make the custom environment inherit from that base class. We can then define a `__name__` for the class.\n",
    "\n",
    "## III. 2. The `reset()` method\n",
    "\n",
    "As we saw above, the reset method initializes the environment for a simulaton episode. This method should be called before any new episode, *not just when creating the class*.\n",
    "\n",
    "When overwriting the `reset()` method, one should ensure that the new method also calls the `environment.effector.reset()` (or to be specific. `self.effector.reset()`) so that the effector states are initialized before they are used by follow-up code.\n",
    "\n",
    "One can then define a custom goal here, and assign it to the `self.goal` attribute so that it is available to every other method. The `self.elapsed` attribute should likely be set to 0. as well, as this attribute keeps track of how much simulation time has elapsed since the beginning of the simulation episode.\n",
    "\n",
    "The `self.obs_buffer` is a dictionary that keeps in memory the several previous states for each entry available. This is usually the proprioception, vision, and action log. Each key is associated with a list containing as many elements as the number of timesteps that this buffer goes back to, in line with the delay properties provided by the user when creting the object instance. It should be `proprioception_delay`, `vision_delay`, and `action_frame_stacking` for proprioception, vision, and action, respectively, and their default values should be 1, 1, and 0, respectively. The 1st item of the list is always the oldest, and the last is the most recent (the instantaneous state).\n",
    "\n",
    "When initializing the observation buffer, we want to use the initial states and fill the buffer with it. To get the vision and proprioception feedback, we can use the `self.get_proprioception()` and `self.get_vision()` methods.\n",
    "\n",
    "\n",
    "We can then get the observation from the `self.get_obs()` method (more on this later), and we set the initial action to be zeros. Finally, we pack all this into the `info` dictionary and return the `(obs, info)` tuple.\n",
    "\n",
    "**Note on seeding:** One can pass a `seed` argument to the `reset()` method to seed the class instance for reproducibility. If the user implements this function to their custom class, they should call the `self._set_generator(seed)` method to initialize a generator with the seed they are passing in as a keyword argument. This will also set a seeded generator to the effector attached to the `Environment` instance. The seed used for the effector will be randomly drawn from the (already seeded) `Environment` generator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyCustomEnv(mn.environment.Environment):\n",
    "  \"\"\"A reach to a random target from a random starting position.\"\"\"\n",
    "\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    # pass everything as-is to the parent Environment class\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.__name__ = \"my_custom_env\"\n",
    "\n",
    "  def reset(self, batch_size: int = 1, joint_state=None, deterministic: bool = False, seed: int | None = None):\n",
    "    \"\"\"\n",
    "    Uses the :meth:`Environment.reset()` method of the parent class :class:`Environment` that can be overwritten to \n",
    "    change the returned data. Here the goals (`i.e.`, the targets) are drawn from a random uniform distribution across\n",
    "    the full joint space.\n",
    "    \"\"\"\n",
    "    self._set_generator(seed)  # seed the environment and the effector\n",
    "  \n",
    "    self.effector.reset(batch_size, joint_state)\n",
    "  \n",
    "    goal = self.joint2cartesian(self.effector.draw_random_uniform_states(batch_size)).chunk(2, dim=-1)[0]\n",
    "    self.goal = goal if self.differentiable else self.detach(goal)\n",
    "    self.elapsed = 0.\n",
    "  \n",
    "    action = th.zeros((batch_size, self.muscle.n_muscles)).to(self.device)\n",
    "  \n",
    "    self.obs_buffer[\"proprioception\"] = [self.get_proprioception()] * len(self.obs_buffer[\"proprioception\"])\n",
    "    self.obs_buffer[\"vision\"] = [self.get_vision()] * len(self.obs_buffer[\"vision\"])\n",
    "    self.obs_buffer[\"action\"] = [action] * self.action_frame_stacking\n",
    "  \n",
    "    obs = self.get_obs(deterministic=deterministic)\n",
    "    info = {\n",
    "      \"states\": self.states,\n",
    "      \"action\": action,\n",
    "      \"noisy action\": action,  # no noise here so it is the same\n",
    "      \"goal\": self.goal,\n",
    "      }\n",
    "    return obs, info\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. 3. The `step()` method\n",
    "\n",
    "\n",
    "The `Environment.step()` method should be called at every step of the episode. It requires an action vector as positional argument, with dimensionality `(batch_size, n_muscles)`. Ideally, a `deterministic` boolean keyword argument should also be present, that defines if noise is applied to various elements of the simulation.\n",
    "\n",
    "This method would usually see the `self.elapsed` attribute updated if the duration of the episode needs to be tracked.\n",
    "\n",
    "The `self.apply_noise()` method will also be useful to those who want to apply noise on various arrays, such as the action input.\n",
    "Note of caution, make sure you don't apply observation noise in the `Environment.step()` method and in the `Environment.get_obs()` method at the same time!\n",
    "\n",
    "Once noise is applied to the action input, it may be passed on to the effector using `self.effector.step(action)`, which will run an integration step of the effector and update the effector's states accordingly. The `Environment.state` attribute will be automatically updated as well to the new states.\n",
    "\n",
    "The goal can also be changed dynamically if the user desires. This can be useful for moving targets or many-targets reaches for instance, where conditional changes should occur. Here the goal is static so we are simply cloning the previous goal.\n",
    "\n",
    "The next observation can be fetched using the `self.get_obs()` method.\n",
    "\n",
    "Finally, the output should be a 5-elements tuple containing the observation vector, the reward information (`None` here as this is only useful for RL), whether the episode is terminated this timestep, whether the episode was terminated early (truncated, always `False` here), and finally the `info` dictionary containing this step's information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MyCustomEnv(mn.environment.Environment):\n",
    "  \"\"\"A reach to a random target from a random starting position.\"\"\"\n",
    "\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    # pass everything as-is to the parent Environment class\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.__name__ = \"my_custom_env\"\n",
    "\n",
    "  def step(self, action, deterministic: bool = False):\n",
    "    \"\"\"\n",
    "    Perform one simulation step. This method is likely to be overwritten by any subclass to implement user-defined \n",
    "    computations, such as reward value calculation for reinforcement learning, custom truncation or termination\n",
    "    conditions, or time-varying goals.\n",
    "    \n",
    "    Args:\n",
    "    action: `Tensor` or `numpy.ndarray`, the input drive to the actuators.\n",
    "    deterministic: `Boolean`, whether observation, action, proprioception, and vision noise are applied.\n",
    "    \n",
    "    Returns:\n",
    "    - The observation vector as `tensor` or `numpy.ndarray`, if the :class:`Environment` is set as differentiable or \n",
    "    not, respectively. It has dimensionality `(batch_size, n_features)`.\n",
    "    - A `numpy.ndarray` with the reward information for the step, with dimensionality `(batch_size, 1)`. This is \n",
    "    `None` if the :class:`Environment` is set as differentiable. By default this always returns `0.` in the \n",
    "    :class:`Environment`.\n",
    "    - A `boolean` indicating if the simulation has been terminated or truncated. If the :class:`Environment` is set as\n",
    "    differentiable, this returns `True` when the simulation time reaches `max_ep_duration` provided at \n",
    "    initialization.\n",
    "    - A `boolean` indicating if the simulation has been truncated early or not. This always returns `False` if the\n",
    "    :class:`Environment` is set as differentiable.\n",
    "    - A `dictionary` containing this step's information.\n",
    "    \"\"\"\n",
    "    \n",
    "    self.elapsed += self.dt\n",
    "    \n",
    "    if deterministic is False:\n",
    "      noisy_action = self.apply_noise(action, noise=self.action_noise)\n",
    "    else:\n",
    "      noisy_action = action\n",
    "    \n",
    "    self.effector.step(noisy_action)\n",
    "    self.goal = self.goal.clone()\n",
    "    \n",
    "    obs = self.get_obs(action=noisy_action)\n",
    "    reward = None\n",
    "    truncated = False\n",
    "    terminated = bool(self.elapsed >= self.max_ep_duration)\n",
    "    info = {\n",
    "      \"states\": self.states,\n",
    "      \"action\": action,\n",
    "      \"noisy action\": noisy_action,\n",
    "      \"goal\": self.goal,\n",
    "      }\n",
    "    return obs, reward, terminated, truncated, info\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. 4. The `get_obs()`, `get_proprioception()`, and `get_vision()` methods\n",
    "\n",
    "The other methods likely to be overwritten are `Environment.get_obs()`, `Environment.get_proprioception()`, and `Environment.get_vision()`.\n",
    "\n",
    "Likely the most important element to add to the `Environment.get_obs()` method is a call to `self.update_obs_buffer()`. This call ensures the buffer is updated with the latest state from the effector and that the oldest state is discarded.\n",
    "\n",
    "The proprioception and vision buffers are updated by fetching the latest effector states via the `Environment.get_proprioception()` and `Environment.get_vision()` methods. Therefore, the user can control what proprioception and vision encompass by overwriting these two methods. They should always return a `(batch_size, n_features)` tensor, with `n_feature` being an arbitrary integer.\n",
    "\n",
    "Once this is done, we can proceed to collecting states from the buffer if we desire. As you may notice, this mainly includes fetching vision and proprioception states, but we can include other elements to it if we wish. Typically, this includes the target position, which for this environment, is available in the `self.goal` attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyCustomEnv(mn.environment.Environment):\n",
    "  \"\"\"A reach to a random target from a random starting position.\"\"\"\n",
    "\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    # pass everything as-is to the parent Environment class\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.__name__ = \"my_custom_env\"\n",
    "\n",
    "  def get_proprioception(self):\n",
    "    \"\"\"\n",
    "    Returns a `(batch_size, n_features)` `tensor` containing the instantaneous (non-delayed) proprioceptive \n",
    "    feedback. By default, this is the normalized muscle length for each muscle, followed by the normalized\n",
    "    muscle velocity for each muscle as well. `.i.i.d.` Gaussian noise is added to each element in the `tensor`,\n",
    "    using the :attribute:`proprioception_noise` attribute.\n",
    "    \"\"\"\n",
    "    mlen = self.states[\"muscle\"][:, 1:2, :] / self.muscle.l0_ce\n",
    "    mvel = self.states[\"muscle\"][:, 2:3, :] / self.muscle.vmax\n",
    "    prop = th.concatenate([mlen, mvel], dim=-1).squeeze(dim=1)\n",
    "    return self.apply_noise(prop, self.proprioception_noise)\n",
    "\n",
    "  def get_vision(self):\n",
    "    \"\"\"\n",
    "    Returns a `(batch_size, n_features)` `tensor` containing the instantaneous (non-delayed) visual \n",
    "    feedback. By default, this is the cartesian position of the end-point effector, that is, the fingertip.\n",
    "    `.i.i.d.` Gaussian noise is added to each element in the `tensor`, using the\n",
    "    :attribute:`vision_noise` attribute.\n",
    "    \"\"\"\n",
    "    vis = self.states[\"fingertip\"]\n",
    "    return self.apply_noise(vis, self.vision_noise)\n",
    "\n",
    "  def get_obs(self, action=None, deterministic: bool = False):\n",
    "    \"\"\"\n",
    "    Returns a `(batch_size, n_features)` `tensor` containing the (potientially time-delayed) observations.\n",
    "    By default, this is the task goal, followed by the output of the :meth:`get_proprioception()` method, \n",
    "    the output of the :meth:`get_vision()` method, and finally the last :attr:`action_frame_stacking` action sets,\n",
    "    if a non-zero `action_frame_stacking` keyword argument was passed at initialization of this class instance.\n",
    "    `.i.i.d.` Gaussian noise is added to each element in the `tensor`,\n",
    "    using the :attribute:`obs_noise` attribute.\n",
    "    \"\"\"\n",
    "    self.update_obs_buffer(action=action)\n",
    "\n",
    "    obs_as_list = [\n",
    "      self.goal,\n",
    "      self.obs_buffer[\"vision\"][0],  # oldest element\n",
    "      self.obs_buffer[\"proprioception\"][0],   # oldest element\n",
    "      ]\n",
    "    \n",
    "    obs = th.cat(obs_as_list, dim=-1)\n",
    "\n",
    "    if deterministic is False:\n",
    "      obs = self.apply_noise(obs, noise=self.obs_noise)\n",
    "\n",
    "    return obs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. 5. Putting it all together\n",
    "\n",
    "Bringing all the above together, this should look like the below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task subclass built.\n",
      "\n",
      "obs shape:           torch.Size([1, 12])\n",
      "\n",
      "\n",
      "states: \n",
      "\t\t\tjoint shape:      torch.Size([1, 4])\n",
      "\t\t\tcartesian shape:  torch.Size([1, 4])\n",
      "\t\t\tmuscle shape:     torch.Size([1, 4, 4])\n",
      "\t\t\tgeometry shape:   torch.Size([1, 4, 4])\n",
      "\t\t\tfingertip shape:  torch.Size([1, 2])\n",
      "action shape:        torch.Size([1, 4])\n",
      "noisy action shape:  torch.Size([1, 4])\n",
      "goal shape:          torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MyCustomEnv(mn.environment.Environment):\n",
    "  \"\"\"A reach to a random target from a random starting position.\"\"\"\n",
    "\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    # pass everything as-is to the parent Environment class\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.__name__ = \"my_custom_env\"\n",
    "\n",
    "  def reset(self, batch_size: int = 1, joint_state=None, deterministic: bool = False, seed: int | None = None):\n",
    "    self._set_generator(seed)  # seed the environment and the effector\n",
    "\n",
    "    self.effector.reset(batch_size, joint_state)\n",
    "  \n",
    "    goal = self.joint2cartesian(self.effector.draw_random_uniform_states(batch_size)).chunk(2, dim=-1)[0]\n",
    "    self.goal = goal if self.differentiable else self.detach(goal)\n",
    "    self.elapsed = 0.\n",
    "    action = th.zeros((batch_size, self.muscle.n_muscles)).to(self.device)\n",
    "  \n",
    "    self.obs_buffer[\"proprioception\"] = [self.get_proprioception()] * len(self.obs_buffer[\"proprioception\"])\n",
    "    self.obs_buffer[\"vision\"] = [self.get_vision()] * len(self.obs_buffer[\"vision\"])\n",
    "    self.obs_buffer[\"action\"] = [action] * self.action_frame_stacking\n",
    "\n",
    "    obs = self.get_obs(deterministic=deterministic)\n",
    "    info = {\n",
    "      \"states\": self.states,\n",
    "      \"action\": action,\n",
    "      \"noisy action\": action,  # no noise here so it is the same\n",
    "      \"goal\": self.goal,\n",
    "      }\n",
    "    return obs, info\n",
    "\n",
    "  def step(self, action, deterministic: bool = False):\n",
    "    self.elapsed += self.dt\n",
    "\n",
    "    if deterministic is False:\n",
    "      noisy_action = self.apply_noise(action, noise=self.action_noise)\n",
    "    else:\n",
    "      noisy_action = action\n",
    "    \n",
    "    self.effector.step(noisy_action)\n",
    "    self.goal = self.goal.clone()\n",
    "\n",
    "    obs = self.get_obs(action=noisy_action)\n",
    "    reward = None\n",
    "    truncated = False\n",
    "    terminated = bool(self.elapsed >= self.max_ep_duration)\n",
    "    info = {\n",
    "      \"states\": self.states,\n",
    "      \"action\": action,\n",
    "      \"noisy action\": noisy_action,\n",
    "      \"goal\": self.goal,\n",
    "      }\n",
    "    return obs, reward, terminated, truncated, info\n",
    "\n",
    "  def get_proprioception(self):\n",
    "    mlen = self.states[\"muscle\"][:, 1:2, :] / self.muscle.l0_ce\n",
    "    mvel = self.states[\"muscle\"][:, 2:3, :] / self.muscle.vmax\n",
    "    prop = th.concatenate([mlen, mvel], dim=-1).squeeze(dim=1)\n",
    "    return self.apply_noise(prop, self.proprioception_noise)\n",
    "\n",
    "  def get_vision(self):\n",
    "    vis = self.states[\"fingertip\"]\n",
    "    return self.apply_noise(vis, self.vision_noise)\n",
    "\n",
    "  def get_obs(self, action=None, deterministic: bool = False):\n",
    "    self.update_obs_buffer(action=action)\n",
    "\n",
    "    obs_as_list = [\n",
    "      self.goal,\n",
    "      self.obs_buffer[\"vision\"][0],  # oldest element\n",
    "      self.obs_buffer[\"proprioception\"][0],   # oldest element\n",
    "      ]\n",
    "    obs = th.cat(obs_as_list, dim=-1)\n",
    "\n",
    "    if deterministic is False:\n",
    "      obs = self.apply_noise(obs, noise=self.obs_noise)\n",
    "    return obs\n",
    "\n",
    "\n",
    "env = MyCustomEnv(effector=mn.effector.ReluPointMass24())\n",
    "print(\"Task subclass built.\\n\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "\n",
    "print(\"obs shape:          \", obs.shape, end=\"\\n\\n\\n\")\n",
    "\n",
    "for key, val in info.items():\n",
    "  if type(val) is dict:\n",
    "    print(key + \": \")\n",
    "    for k, v in val.items():\n",
    "      print(\"\\t\\t\\t\" + k + \" shape:\" + \" \" * (10-len(k)), v.shape)\n",
    "  else:\n",
    "    print(key + \" shape:\" + \" \" * (13-len(key)), val.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs shape:           torch.Size([1, 12])\n",
      "reward:              None\n",
      "terminated:          False\n",
      "truncated:           False\n",
      "\n",
      "info:\n",
      "\tstates: \n",
      "\t\tjoint shape:      torch.Size([1, 4])\n",
      "\t\tcartesian shape:  torch.Size([1, 4])\n",
      "\t\tmuscle shape:     torch.Size([1, 4, 4])\n",
      "\t\tgeometry shape:   torch.Size([1, 4, 4])\n",
      "\t\tfingertip shape:  torch.Size([1, 2])\n",
      "\taction shape:        torch.Size([1, 4])\n",
      "\tnoisy action shape:  torch.Size([1, 4])\n",
      "\tgoal shape:          torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "obs, reward, terminated, truncated, info = env.step(action=th.zeros(1, env.n_muscles))\n",
    "\n",
    "print(\"obs shape:          \", obs.shape)\n",
    "print(\"reward:             \", reward)\n",
    "print(\"terminated:         \", terminated)\n",
    "print(\"truncated:          \", truncated, end=\"\\n\\n\")\n",
    "\n",
    "print(\"info:\")\n",
    "for key, val in info.items():\n",
    "  if type(val) is dict:\n",
    "    print(\"\\t\" + key + \": \")\n",
    "    for k, v in val.items():\n",
    "      print(\"\\t\\t\" + k + \" shape:\" + \" \" * (10-len(k)), v.shape)\n",
    "  else:\n",
    "    print(\"\\t\" + key + \" shape:\" + \" \" * (13-len(key)), val.shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
